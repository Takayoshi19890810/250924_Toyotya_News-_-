import os
import json
import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# ========== Google Sheets æ¥ç¶š ==========
def get_gspread_client():
    credentials_json_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
    if not credentials_json_str:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    credentials_dict = json.loads(credentials_json_str)
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    gc = gspread.authorize(credentials)
    return gc


# ========== Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ==========
def scrape_yahoo_news(keyword: str, limit: int = 50):
    print(f"ğŸ” Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹: {keyword}")

    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    headers = {"User-Agent": "Mozilla/5.0"}

    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {res.status_code}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    articles = []

    no = 1
    for item in soup.select("li a"):
        href = item.get("href")
        title = item.get_text(strip=True)

        # Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹URLã®ã¿å¯¾è±¡
        if not href or "news.yahoo.co.jp" not in href:
            continue

        # å¼•ç”¨å…ƒã‚„ç™ºè¡Œæ—¥æ™‚ã®å–å¾—ï¼ˆæ¤œç´¢çµæœã®æ§‹é€ ã«ã‚ˆã‚‹ï¼‰
        parent = item.find_parent("li")
        source = parent.select_one("span").get_text(strip=True) if parent and parent.select_one("span") else ""
        pubdate = parent.select_one("time").get_text(strip=True) if parent and parent.select_one("time") else ""

        row = [
            no,         # No.
            title,      # ã‚¿ã‚¤ãƒˆãƒ«
            href,       # URL
            source,     # å¼•ç”¨å…ƒ
            pubdate,    # ç™ºè¡Œæ—¥æ™‚
            "",         # ãƒã‚¸ãƒã‚¬ï¼ˆå¾Œã§åˆ†æç”¨ï¼‰
            "",         # ã‚«ãƒ†ã‚´ãƒªï¼ˆå¾Œã§åˆ†é¡ç”¨ï¼‰
            "",         # æœ¬æ–‡ï¼ˆè¨˜äº‹è©³ç´°ã‚’åˆ¥é€”å–å¾—ã™ã‚‹å¿…è¦ã‚ã‚Šï¼‰
            "",         # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆåˆ¥å‡¦ç†ï¼‰
            ""          # ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆåˆ¥å‡¦ç†ï¼‰
        ]
        articles.append(row)
        print(f"{no}. {title} ({href})")
        no += 1

        if len(articles) >= limit:
            break

    print(f"âœ… {len(articles)} ä»¶å–å¾—")
    return articles


# ========== ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ ==========
def write_to_sheet(sh, keyword: str, articles: list):
    sheet_name = keyword
    try:
        worksheet = sh.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = sh.add_worksheet(title=sheet_name, rows="2000", cols="10")

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
    headers = [
        "No.", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "å¼•ç”¨å…ƒ", "ç™ºè¡Œæ—¥æ™‚",
        "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆ"
    ]
    worksheet.update("A1:J1", [headers])

    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
    if articles:
        worksheet.update(f"A2:J{len(articles)+1}", articles)


# ========== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==========
def main():
    keyword = "ãƒˆãƒ¨ã‚¿"  # å›ºå®š
    spreadsheet_id = os.environ.get("SPREADSHEET_ID")

    if not spreadsheet_id:
        raise RuntimeError("SPREADSHEET_ID ãŒæœªè¨­å®šã§ã™ã€‚")

    gc = get_gspread_client()
    sh = gc.open_by_key(spreadsheet_id)

    articles = scrape_yahoo_news(keyword, limit=50)

    if articles:
        write_to_sheet(sh, keyword, articles)
        print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        print("âš ï¸ è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()
