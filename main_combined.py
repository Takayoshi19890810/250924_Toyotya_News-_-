import os
import re
import json
import time
import requests
from typing import List, Tuple
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By


# ==========================
# Google Sheets æ¥ç¶š
# ==========================
def get_gspread_client():
    key_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
    if not key_str:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    creds_dict = json.loads(key_str)
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    return gspread.authorize(creds)


# ==========================
# Chromium/Driver ã®ãƒ‘ã‚¹è§£æ±ºï¼ˆç’°å¢ƒå·®å¸åï¼‰
# ==========================
def resolve_chrome_paths() -> Tuple[str, str]:
    # chromium / chromium-browser / google-chrome ã®é †ã«æ¢ç´¢
    chrome_candidates = [
        "/usr/bin/chromium-browser",
        "/usr/bin/chromium",
        "/usr/bin/google-chrome",
    ]
    chrome_binary = next((p for p in chrome_candidates if os.path.exists(p)), None)
    if not chrome_binary:
        raise RuntimeError("Chrome/Chromium å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # chromedriver ã¯ä¸€èˆ¬ã«ã“ã“
    driver_candidates = ["/usr/bin/chromedriver", "/snap/bin/chromium.chromedriver"]
    driver_binary = next((p for p in driver_candidates if os.path.exists(p)), None)
    if not driver_binary:
        raise RuntimeError("chromedriver ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    return chrome_binary, driver_binary


def create_driver():
    chrome_binary, driver_binary = resolve_chrome_paths()
    chrome_options = Options()
    chrome_options.binary_location = chrome_binary
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=ja-JP")
    chrome_options.add_argument("--window-size=1280,1800")

    service = Service(driver_binary)
    return webdriver.Chrome(service=service, options=chrome_options)


# ==========================
# HTTP ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆç°¡æ˜“ãƒªãƒˆãƒ©ã‚¤ï¼‰
# ==========================
UA_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
}

def get_with_retry(url: str, headers=None, params=None, tries=3, sleep=1.0):
    headers = headers or UA_HEADERS
    for i in range(tries):
        r = requests.get(url, headers=headers, params=params, timeout=20)
        if r.status_code == 200:
            return r
        time.sleep(sleep)
    return r  # æœ€å¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™


# ==========================
# ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä»¶å–å¾—ï¼ˆcursor ã§ãƒšãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
# ==========================
ARTICLE_ID_RE = re.compile(r"/articles/([0-9A-Za-z]+)")

def fetch_all_comments(article_url: str) -> List[str]:
    m = ARTICLE_ID_RE.search(article_url)
    if not m:
        return []
    article_id = m.group(1)
    base = f"https://news.yahoo.co.jp/comment/plugin/v1/full/{article_id}"

    comments: List[str] = []
    cursor = None

    while True:
        params = {"sort": "time", "count": 100}
        if cursor:
            params["cursor"] = cursor

        res = get_with_retry(base, headers=UA_HEADERS, params=params)
        if res.status_code != 200:
            break

        try:
            data = res.json()
            result = data.get("result", {})
            batch = [c.get("comment", "") for c in result.get("comments", [])]
            comments.extend(batch)
            cursor = result.get("next")
            if not cursor:
                break
        except Exception:
            break

    return comments


# ==========================
# è¨˜äº‹æœ¬æ–‡ãƒ»ç™ºè¡Œæ—¥æ™‚ãƒ»å¼•ç”¨å…ƒ
# ==========================
def fetch_article_details(url: str):
    res = get_with_retry(url, headers=UA_HEADERS)
    if res.status_code != 200:
        return "", "", "", []

    soup = BeautifulSoup(res.text, "html.parser")

    # æœ¬æ–‡ã¯ãƒšãƒ¼ã‚¸å‹ã§è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ãŸã‚å€™è£œã‚’é †ã«æ‹¾ã†
    body_sel_candidates = [
        "div.article_body p",
        "article p",
        "div.yjS p",
        "div.article_body",
    ]
    body_texts = []
    for sel in body_sel_candidates:
        nodes = soup.select(sel)
        if nodes:
            body_texts = [n.get_text(strip=True) for n in nodes]
            break
    body = " ".join([t for t in body_texts if t])

    # å¼•ç”¨å…ƒãƒ»ç™ºè¡Œæ—¥æ™‚å€™è£œ
    source = ""
    pubdate = ""
    if soup.select_one("span.source"):
        source = soup.select_one("span.source").get_text(strip=True)
    elif soup.select_one("a[href*='news.yahoo.co.jp/publish']"):
        source = soup.select_one("a[href*='news.yahoo.co.jp/publish']").get_text(strip=True)

    if soup.select_one("time"):
        pubdate = soup.select_one("time").get_text(strip=True)

    comments = fetch_all_comments(url)
    return body, source, pubdate, comments


# ==========================
# æ¤œç´¢çµæœ ã™ã¹ã¦å–å¾—ï¼ˆãƒšãƒ¼ã‚¸é€ã‚Šï¼‰
# ==========================
def scrape_all_article_urls(keyword: str) -> List[str]:
    driver = create_driver()
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(search_url)
    time.sleep(2)

    urls = []
    seen_pages = 0
    while True:
        # ãã®ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’å›å
        links = driver.find_elements(By.CSS_SELECTOR, "a")
        for e in links:
            href = e.get_attribute("href")
            if href and "news.yahoo.co.jp/articles/" in href:
                urls.append(href)

        # æ¬¡ã¸ï¼ˆã¾ãŸã¯ã€Œæ¬¡ã®ãƒšãƒ¼ã‚¸ã€ï¼‰ã‚’ã‚¯ãƒªãƒƒã‚¯
        # ãƒšãƒ¼ã‚¸ã®DOMã¯å¤‰ã‚ã‚Šã‚„ã™ã„ã®ã§è¤‡æ•°å€™è£œã‚’è©¦ã™
        clicked = False
        next_selectors = [
            "a.Pagination__next",
            "a[aria-label='æ¬¡ã¸']",
            "a:contains('æ¬¡ã¸')",  # JSDOMäº’æ›ã§ã¯ãªã„ãŒä¿é™º
        ]
        for sel in next_selectors:
            try:
                # CSS :contains ã¯ Selenium ã§ã¯ä½¿ãˆãªã„ãŸã‚ By.LINK_TEXT ã‚‚è©¦ã™
                if sel == "a:contains('æ¬¡ã¸')":
                    elem = driver.find_element(By.LINK_TEXT, "æ¬¡ã¸")
                else:
                    elem = driver.find_element(By.CSS_SELECTOR, sel)
                driver.execute_script("arguments[0].click();", elem)
                time.sleep(1.5)
                clicked = True
                break
            except Exception:
                continue

        seen_pages += 1
        if not clicked or seen_pages > 100:  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            break

    driver.quit()
    # é‡è¤‡æ’é™¤
    return list(dict.fromkeys(urls))


# ==========================
# ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ï¼ˆè¿½è¨˜ãƒ»é‡è¤‡å›é¿ï¼‰
# ==========================
HEADERS = [
    "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "å¼•ç”¨å…ƒ", "ç™ºè¡Œæ—¥æ™‚",
    "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "æœ¬æ–‡", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆ"
]

def ensure_worksheet_with_headers(sh, sheet_name: str):
    try:
        ws = sh.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=sheet_name, rows="200000", cols="9")

    # ãƒ˜ãƒƒãƒ€æœªè¨­å®šãªã‚‰è¨­å®š
    cell_a1 = ws.acell("A1").value
    if not cell_a1:
        ws.update("A1:I1", [HEADERS])
    return ws


def load_existing_url_comment_pairs(ws) -> set:
    """
    æ—¢å­˜ã® (URL, ã‚³ãƒ¡ãƒ³ãƒˆ) ã®çµ„ã¿åˆã‚ã›é›†åˆã‚’å–å¾—ã€‚
    Båˆ—=URL, Iåˆ—=ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ2è¡Œç›®ä»¥é™ï¼‰ã‚’èª­ã¿è¾¼ã‚€ã€‚
    """
    # Båˆ—ã¨Iåˆ—ã‚’ã¾ã¨ã‚ã¦å–å¾—ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ã ã¨æ™‚é–“ãŒã‹ã‹ã‚‹ç‚¹ã«æ³¨æ„ï¼‰
    # è¡Œæ•°ã‚’ã¾ãšå–å¾—
    last_row = len(ws.col_values(2))  # Båˆ—ã®æœ€çµ‚è¡Œã‚’åŸºæº–
    if last_row <= 1:
        return set()

    rng = ws.get(f"B2:B{last_row}")  # URL
    rng2 = ws.get(f"I2:I{last_row}")  # ã‚³ãƒ¡ãƒ³ãƒˆ
    existing = set()
    # get ã¯ [[val], [val], ...] ã®å½¢
    for (u_row, c_row) in zip(rng, rng2):
        u = u_row[0] if u_row else ""
        c = c_row[0] if c_row else ""
        if u or c:
            existing.add((u, c))
    return existing


def append_rows_dedup(ws, rows: List[List[str]]):
    """
    æ—¢å­˜ (URL, ã‚³ãƒ¡ãƒ³ãƒˆ) ã¨é‡è¤‡ã—ãªã„è¡Œã®ã¿ã‚’æœ«å°¾ã«è¿½è¨˜ã€‚
    è¿½è¨˜ã¯ 5000 è¡Œãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    existing_pairs = load_existing_url_comment_pairs(ws)
    to_append = []
    for r in rows:
        url = r[1] if len(r) > 1 else ""
        comment = r[8] if len(r) > 8 else ""
        pair = (url, comment)
        if pair not in existing_pairs:
            to_append.append(r)

    if not to_append:
        print("è¿½è¨˜å¯¾è±¡ã®æ–°è¦è¡Œã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦é‡è¤‡ï¼‰ã€‚")
        return

    # è¿½è¨˜ï¼šappend_rows ã‚’ãƒãƒ£ãƒ³ã‚¯ã§
    CHUNK = 5000
    for i in range(0, len(to_append), CHUNK):
        chunk = to_append[i:i + CHUNK]
        ws.append_rows(chunk, value_input_option="RAW")
        print(f"è¿½è¨˜: {i+1}ï½{i+len(chunk)} è¡Œ")


# ==========================
# ãƒ¡ã‚¤ãƒ³
# ==========================
def main():
    keyword = "ãƒˆãƒ¨ã‚¿"  # å›ºå®š
    spreadsheet_id = os.environ.get("SPREADSHEET_ID")
    if not spreadsheet_id:
        raise RuntimeError("SPREADSHEET_ID ãŒæœªè¨­å®šã§ã™ã€‚")

    # Sheets
    gc = get_gspread_client()
    sh = gc.open_by_key(spreadsheet_id)
    ws = ensure_worksheet_with_headers(sh, keyword)

    # æ¤œç´¢ â†’ å…¨ãƒšãƒ¼ã‚¸åˆ†ã®è¨˜äº‹URL
    print(f"ğŸ” Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹: {keyword}")
    urls = scrape_all_article_urls(keyword)
    print(f"ğŸ“‘ åé›†URLæ•°: {len(urls)}")

    # å„è¨˜äº‹ â†’ æœ¬æ–‡ & ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä»¶
    rows: List[List[str]] = []
    for idx, href in enumerate(urls, 1):
        body, source, pubdate, comments = fetch_article_details(href)
        comment_count = len(comments)
        title = f"[{keyword}] {href.split('/')[-1]}"  # ã‚¿ã‚¤ãƒˆãƒ«ã¯ç°¡æ˜“ï¼ˆå¿…è¦ãªã‚‰è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã«å¤‰æ›´ï¼‰

        if comments:
            for c in comments:
                rows.append([
                    title,
                    href,
                    source,
                    pubdate,
                    "",  # ãƒã‚¸ãƒã‚¬
                    "",  # ã‚«ãƒ†ã‚´ãƒª
                    body,
                    comment_count,
                    c
                ])
        else:
            rows.append([
                title, href, source, pubdate, "", "", body, 0, ""
            ])

        print(f"{idx}/{len(urls)}: {href} ã‚³ãƒ¡ãƒ³ãƒˆæ•°={comment_count}")

    # è¿½è¨˜ï¼ˆé‡è¤‡å›é¿ï¼‰
    append_rows_dedup(ws, rows)
    print("âœ… å®Œäº†ï¼šã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
